{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def sample_data(input_file, output_file, target_size_gb, filter_key='also_buy'):\n",
    "    target_size_bytes = target_size_gb * 1024 * 1024 * 1024\n",
    "    current_size_bytes = 0\n",
    "\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for line in tqdm(infile):\n",
    "            record = json.loads(line)\n",
    "            if record.get(filter_key):\n",
    "                outfile.write(json.dumps(record) + '\\n')\n",
    "                current_size_bytes += len(line.encode('utf-8'))\n",
    "            \n",
    "            if current_size_bytes >= target_size_bytes:\n",
    "                break\n",
    "    print(f\"Finished sampling data to {output_file}, Output size: {current_size_bytes / 1024 / 1024 / 1024} GB\")\n",
    "\n",
    "    sample_data('Sampled_Amazon_Meta.json', 'Data.json', .003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Helper function to clean and normalize text data\n",
    "def clean_text(text):\n",
    "    # Remove HTML tags\n",
    "    clean = re.sub(r'<.*?>', '', text)\n",
    "    # Convert to lowercase\n",
    "    return clean.lower()\n",
    "\n",
    "# Load the JSON data from the file\n",
    "def load_data(file_path):\n",
    "    data_entries = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                # Parse each line as a JSON object\n",
    "                json_entry = json.loads(line)\n",
    "                data_entries.append(json_entry)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e} - Line: {line}\")\n",
    "    return data_entries\n",
    "\n",
    "# Preprocess the data\n",
    "def preprocess_data(data_entries):\n",
    "    transactions = []\n",
    "    for entry in data_entries:\n",
    "        transaction = []\n",
    "        if 'category' in entry and isinstance(entry['category'], list):\n",
    "            transaction.extend([clean_text(item) for item in entry['category']])\n",
    "        if 'also_view' in entry and isinstance(entry['also_view'], list):\n",
    "            transaction.extend(entry['also_view'])\n",
    "        if transaction:\n",
    "            transactions.append(set(transaction))\n",
    "    return transactions\n",
    "\n",
    "# Save the processed data into a new JSON file\n",
    "def save_transactions_to_json(transactions, output_file_path):\n",
    "    transactions_as_lists = [list(transaction) for transaction in transactions]\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        json.dump(transactions_as_lists, output_file, indent=4)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "file_path = 'Sampled_Amazon_Meta.json'\n",
    "output_file_path = 'Processed.json'\n",
    "\n",
    "data_entries = load_data(file_path)\n",
    "transactions = preprocess_data(data_entries)\n",
    "\n",
    "save_transactions_to_json(transactions, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 69\u001b[0m\n\u001b[1;32m     67\u001b[0m min_support \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     68\u001b[0m min_confidence \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m---> 69\u001b[0m frequent_sets \u001b[38;5;241m=\u001b[39m \u001b[43mfrequent_item_sets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_support\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m rules \u001b[38;5;241m=\u001b[39m generate_rules(frequent_sets, min_confidence)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Print the results\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 42\u001b[0m, in \u001b[0;36mfrequent_item_sets\u001b[0;34m(transactions, min_support)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     L_k_minus_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(itemsets[k\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m---> 42\u001b[0m     C_k \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL_k_minus_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     C_k_counts \u001b[38;5;241m=\u001b[39m count_support(C_k, transactions)\n\u001b[1;32m     44\u001b[0m     L_k \u001b[38;5;241m=\u001b[39m {cand: count \u001b[38;5;28;01mfor\u001b[39;00m cand, count \u001b[38;5;129;01min\u001b[39;00m C_k_counts\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m count \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m min_support}\n",
      "Cell \u001b[0;32mIn[2], line 17\u001b[0m, in \u001b[0;36mgenerate_candidates\u001b[0;34m(L_k_minus_1, k)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(L_k_minus_1_list)):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(L_k_minus_1_list)):\n\u001b[0;32m---> 17\u001b[0m         candidate \u001b[38;5;241m=\u001b[39m \u001b[43mL_k_minus_1_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL_k_minus_1_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(candidate) \u001b[38;5;241m==\u001b[39m k:\n\u001b[1;32m     19\u001b[0m             candidates\u001b[38;5;241m.\u001b[39madd(candidate)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "\n",
    "# Helper function to load transactions from a JSON file\n",
    "def load_transactions(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        transactions = json.load(file)\n",
    "    return [set(transaction) for transaction in transactions]\n",
    "\n",
    "# Generate candidate itemsets from the frequent itemsets of size k-1\n",
    "def generate_candidates(L_k_minus_1, k):\n",
    "    candidates = set()\n",
    "    L_k_minus_1_list = list(L_k_minus_1)\n",
    "    for i in range(len(L_k_minus_1_list)):\n",
    "        for j in range(i + 1, len(L_k_minus_1_list)):\n",
    "            candidate = L_k_minus_1_list[i].union(L_k_minus_1_list[j])\n",
    "            if len(candidate) == k:\n",
    "                candidates.add(candidate)\n",
    "    return candidates\n",
    "\n",
    "# Count the support for each candidate itemset\n",
    "def count_support(candidates, transactions):\n",
    "    support_count = defaultdict(int)\n",
    "    for candidate in candidates:\n",
    "        for transaction in transactions:\n",
    "            if candidate.issubset(transaction):\n",
    "                support_count[candidate] += 1\n",
    "    return support_count\n",
    "\n",
    "# Generate all frequent item sets with the given minimum support\n",
    "def frequent_item_sets(transactions, min_support):\n",
    "    itemsets = []\n",
    "    single_items = set(item for transaction in transactions for item in transaction)\n",
    "    single_item_counts = count_support([frozenset([item]) for item in single_items], transactions)\n",
    "    L_1 = {item: count for item, count in single_item_counts.items() if count >= min_support}\n",
    "    itemsets.append(L_1)\n",
    "    \n",
    "    k = 2\n",
    "    while True:\n",
    "        L_k_minus_1 = set(itemsets[k-2].keys())\n",
    "        C_k = generate_candidates(L_k_minus_1, k)\n",
    "        C_k_counts = count_support(C_k, transactions)\n",
    "        L_k = {cand: count for cand, count in C_k_counts.items() if count >= min_support}\n",
    "        if not L_k:\n",
    "            break\n",
    "        itemsets.append(L_k)\n",
    "        k += 1\n",
    "    return itemsets\n",
    "\n",
    "# Generate association rules from the frequent itemsets\n",
    "def generate_rules(itemsets, min_confidence):\n",
    "    rules = []\n",
    "    for k in range(1, len(itemsets)):\n",
    "        for itemset, support in itemsets[k].items():\n",
    "            for consequence in combinations(itemset, 1):\n",
    "                antecedent = itemset - set(consequence)\n",
    "                antecedent_support = itemsets[len(antecedent)-1][frozenset(antecedent)]\n",
    "                confidence = support / antecedent_support\n",
    "                if confidence >= min_confidence:\n",
    "                    rules.append((antecedent, consequence, confidence))\n",
    "    return rules\n",
    "\n",
    "# Example usage\n",
    "file_path = 'Processed.json'  # Update with the correct path\n",
    "transactions = load_transactions(file_path)\n",
    "min_support = 2\n",
    "min_confidence = 0.5\n",
    "frequent_sets = frequent_item_sets(transactions, min_support)\n",
    "rules = generate_rules(frequent_sets, min_confidence)\n",
    "\n",
    "# Print the results\n",
    "print(\"Frequent Item Sets:\")\n",
    "for level in frequent_sets:\n",
    "    print(level)\n",
    "print(\"\\nAssociation Rules:\")\n",
    "for rule in rules:\n",
    "    print(rule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
